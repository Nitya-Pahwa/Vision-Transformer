{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b805d2-e94c-4e70-a333-717f6c1258f0",
   "metadata": {},
   "source": [
    "VIT Transformer\n",
    "Vit Transformer was used for sequence to sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a169b9c3-18da-45a1-a970-8dda4c74d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import datetime\n",
    "import json\n",
    "import seaborn as sns\n",
    "from contextlib import nullcontext\n",
    "import matplotlib.pyplot as plt\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5319a5e-f0ea-4c35-b781-134caa493e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b70d1cce-1096-4e6c-a08d-4fabb52bd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_statistics(model, input_size=(3, 224, 224), device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Compute FLOPs, total parameters, estimated memory usage, and average inference time.\n",
    "    Requires the 'ptflops' package.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from ptflops import get_model_complexity_info\n",
    "        with torch.cuda.device(0) if device == \"cuda\" else nullcontext():\n",
    "            flops, ptflops_params = get_model_complexity_info(\n",
    "                model, input_size, as_strings=True,\n",
    "                print_per_layer_stat=False, verbose=False\n",
    "            )\n",
    "    except ImportError:\n",
    "        flops, ptflops_params = \"N/A\", \"N/A\"\n",
    "        print(\"ptflops package not found. Skipping FLOPs calculation.\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    estimated_memory_usage_bytes = total_params * 4  # assuming float32 (4 bytes per parameter)\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device)\n",
    "    model.eval()\n",
    "    # Warm-up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "    # Measure inference time over 100 runs.\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            _ = model(dummy_input)\n",
    "    end_time = time.time()\n",
    "    avg_inference_time = (end_time - start_time) / 100\n",
    "\n",
    "    stats = {\n",
    "        \"flops\": flops,\n",
    "        \"ptflops_params\": ptflops_params,\n",
    "        \"total_params\": total_params,\n",
    "        \"estimated_memory_usage_bytes\": estimated_memory_usage_bytes,\n",
    "        \"avg_inference_time_seconds\": avg_inference_time\n",
    "    }\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c83214df-5f1a-413c-8535-e8e17116560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true,y_pred,y_score=None):\n",
    "    metrics={}\n",
    "    metrics[\"accuracy_score\"]=accuracy_score(y_pred,y_true)\n",
    "    metrics[\"top_1_accuracy\"]=metrics[\"accuracy_score\"]\n",
    "    if y_score is not None and y_score.shape[1]>=3:\n",
    "        top3_correct=0\n",
    "        for i,true_label in enumerate(y_true):\n",
    "            top3_indices=np.argsort(y_score[i])[::-1][:3] \n",
    "            if true_label in top3_indices:\n",
    "                top3_correct+=1\n",
    "            metrics[\"top_3_accuracy\"]=top3_correct/len(y_true)\n",
    "            \n",
    "    else:\n",
    "        if y_score is not None and y_score.shape[1]<3:\n",
    "            print(\"Less than 3 classes so top_3 accurcay will be same as top_1 accurcay\")\n",
    "            metrics[\"top_3_accuracy\"]=metrics[\"top_1 accuracy\"]\n",
    "        else:\n",
    "            metrics[\"top_3_accuracy\"]=None\n",
    "    # Precision\n",
    "    metrics['precision_micro']=precision_score(y_true,y_pred,average='micro',zero_division=0)\n",
    "    metrics['precision_macro']=precision_score(y_true,y_pred,average='macro',zero_division=0)\n",
    "    metrics['precision_weighted']=precision_score(y_true,y_pred,average='weighted',zero_division=0)\n",
    "    \n",
    "    # Recall\n",
    "    metrics['recall_micro']=recall_score(y_true,y_pred,average='micro',zero_division=0)\n",
    "    metrics['recall_macro']=recall_score(y_true,y_pred,average='macro',zero_division=0)\n",
    "    metrics['recall_weighted']=recall_score(y_true,y_pred,average='weighted',zero_division=0)\n",
    "    \n",
    "    # F1 Score\n",
    "    metrics['f1_micro']=f1_score(y_true,y_pred,average='micro',zero_division=0)\n",
    "    metrics['f1_macro']=f1_score(y_true,y_pred,average='macro',zero_division=0)\n",
    "    metrics['f1_weighted']=f1_score(y_true,y_pred,average='weighted',zero_division=0)\n",
    "\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            # One-hot encode the true labels for multi-class ROC AUC\n",
    "            y_true_onehot = np.zeros((len(y_true),len(np.unique(y_true))))\n",
    "            for i, val in enumerate(y_true):\n",
    "                y_true_onehot[i,val]=1\n",
    "                \n",
    "            metrics['auc_micro']=roc_auc_score(y_true_onehot,y_score,average='micro',multi_class='ovr')\n",
    "            metrics['auc_macro']=roc_auc_score(y_true_onehot,y_score,average='macro',multi_class='ovr')\n",
    "            metrics['auc_weighted']=roc_auc_score(y_true_onehot,y_score,average='weighted',multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate AUC metrics: {e}\")\n",
    "            metrics['auc_micro']=metrics['auc_macro']=metrics['auc_weighted']=None\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906e8fee-4fbe-43ca-8ae2-910ef2e90af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(save_dir):\n",
    "    checkpoint_files = glob.glob(os.path.join(save_dir, \"checkpoint_epoch_*.pth\"))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    return max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load checkpoint and return the starting epoch.\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return checkpoint['epoch'] + 1  # Return next epoch to start from\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adfee865-750d-4a5c-9ca2-fc55a882be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"SoyMCData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a460630a-56e0-4c0f-9e45-8da439d1595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d611e83d-a7d8-42a9-81c6-1529bf7389e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=datasets.ImageFolder(os.path.join(dir,\"train\"),transform=transform)\n",
    "test_dataset=datasets.ImageFolder(os.path.join(dir,\"test\"),transform=transform)\n",
    "val_dataset=datasets.ImageFolder(os.path.join(dir,\"val\"),transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "910e652e-ff3b-4059-97c5-04914e3f194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bcb3e8b-76e5-4d8a-aed0-c8b0f27fda41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc4fdc31-efa2-4122-b6e1-7c5f9bcc2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=False)\n",
    "val_loader=DataLoader(val_dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5700a-a6bd-4a5c-b5e2-537537229ea3",
   "metadata": {},
   "source": [
    "Patch Embedding Layer\n",
    "Since we want to train VIT Transformer on Images we need to convert images to set of sequence to sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d87af6fc-7487-4e3c-bf40-043cf5ae2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,d_model,img_size,patch_size,n_channels):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.img_size=img_size\n",
    "        self.patch_size=patch_size\n",
    "        self.n_channels=n_channels\n",
    "\n",
    "        # converting the image into patches of data \n",
    "        self.layer=nn.Conv2d(in_channels=n_channels,out_channels=d_model,kernel_size=self.patch_size,stride=self.patch_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.layer(x)\n",
    "        # Flattening the layer\n",
    "        x=x.flatten(2)\n",
    "        # Transposing the flatten version of the layer\n",
    "        x=x.transpose(1,2)\n",
    "        return x;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6140ba-aa15-4a0e-8af2-52f7610730f9",
   "metadata": {},
   "source": [
    "Positional Embedding \n",
    "Now the problem is that Images are converted into patches of data but how can we determine the position of a patch in an image?\n",
    "So in order to solve the above problem the concept of Positional Encoding was introduced it adds a value that can tell the position of a particular patch in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36e1a0e7-0b23-4575-b0be-f4bf7d72251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_sequence_length,d_model):\n",
    "        super().__init__()\n",
    "        # cls token is added -> it tells \n",
    "        self.cls_token=nn.Parameter(torch.randn(1,1,d_model))\n",
    "        \n",
    "        # Positional Embedding\n",
    "        pe=torch.zeros(max_sequence_length,d_model)\n",
    "        \n",
    "        for pos in range(max_sequence_length):\n",
    "          for i in range(d_model):\n",
    "            if i % 2 == 0:\n",
    "              pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
    "            else:\n",
    "              pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
    "                \n",
    "        # since position embedding layer is fixed i.e not trainable so making it a buffer\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Expand to have class token for every image in batch\n",
    "        tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
    "    \n",
    "        # Adding class tokens to the beginning of each embedding\n",
    "        x = torch.cat((tokens_batch,x), dim=1)\n",
    "    \n",
    "        # Add positional embedding to embeddings\n",
    "        x = x + self.pe\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa684cd-38e7-4aa4-8d71-41f28fd28441",
   "metadata": {},
   "source": [
    "Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "684147a0-82a6-4873-a9cf-88c64145bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self,d_model,head_size):\n",
    "        super().__init__()\n",
    "        self.head_size=head_size\n",
    "        # calculating the query matrix\n",
    "        self.query=nn.Linear(d_model,head_size)\n",
    "        # calculating the key matrix        \n",
    "        self.key=nn.Linear(d_model,head_size)\n",
    "        # calculating the value matrix\n",
    "        self.value=nn.Linear(d_model,head_size)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q=self.query(x)\n",
    "        K=self.key(x)\n",
    "        V=self.value(x)\n",
    "\n",
    "        # calculating the dot product of query and key transpose (QKt)\n",
    "        attention=Q@K.transpose(-2,-1)\n",
    "\n",
    "        # scaling the attention bu 1/root(d)\n",
    "        attention=attention/(self.head_size ** 0.5)\n",
    "\n",
    "        # applying softmax\n",
    "        attention=torch.softmax(attention,dim=-1)\n",
    "\n",
    "        # calculating dot product of attention and V \n",
    "        attention=attention@V\n",
    "        \n",
    "        return attention;\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34966a-e042-4f6d-ae74-4184e55bda5a",
   "metadata": {},
   "source": [
    "Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65aed0d3-0363-4982-9584-dab1a09f938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size=d_model//n_head\n",
    "\n",
    "        # calculating the Wo matrix\n",
    "        self.W_o =nn.Linear(d_model,d_model)\n",
    "\n",
    "        # calculating all the heads by stacking multiple(n_head) Attention Head\n",
    "        self.heads=nn.ModuleList([AttentionHead(d_model,self.head_size) for _ in range(n_head)])\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # concatinating all the heads\n",
    "        out =torch.cat([head(x) for head in self.heads],dim=-1)\n",
    "\n",
    "        # calculating multi head attention (Z dot proct W_o) where Z is concatination of all the heads\n",
    "        out=self.W_o(out)\n",
    "        \n",
    "        return out;\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b81693-30b3-4e30-8eb6-e23dd45ccbd7",
   "metadata": {},
   "source": [
    "Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36762a9b-7e6e-4d9e-82b2-4575d9f04256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,d_model,n_head,r_mlp=4):\n",
    "        # r_mlp is an expansion factor/ratio used in stpes of mlp\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_head=n_head\n",
    "\n",
    "        # Layer normalization layer 1\n",
    "        self.ln1=nn.LayerNorm(d_model)\n",
    "\n",
    "        # Multi Head Attention Layer\n",
    "        self.mha=MultiHeadAttention(d_model,n_head)\n",
    "\n",
    "        # Layer Normalization layer 2 \n",
    "        self.ln2=nn.LayerNorm(d_model)\n",
    "\n",
    "        # Multi Layer Perceptron (MLP) / Feed Forward Neural Network (FFN)\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(d_model,d_model*r_mlp), # expanding dimensionality\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model*r_mlp,d_model) # bring dimensionality back to original \n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.ln1(x)\n",
    "        # Residual connection for layer 1 \n",
    "        out=x+self.mha(x)\n",
    "\n",
    "        out=self.ln2(out)\n",
    "        # Residual connection for layer 2\n",
    "        out=out+self.mlp(out)\n",
    "        \n",
    "        return out;\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dda167-5704-4a56-b5d3-95a540f3aba5",
   "metadata": {},
   "source": [
    "Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "233785bf-9578-4067-b748-cf4725ac8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vision(nn.Module):\n",
    "    def __init__(self,d_model,n_classes,img_size,patch_size,n_channels,n_heads,n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model=d_model\n",
    "        self.n_classes=n_classes\n",
    "        self.img_size=img_size\n",
    "        self.patch_size=patch_size\n",
    "        self.n_channels=n_channels\n",
    "        self.n_heads=n_heads\n",
    "        self.n_layers=n_layers\n",
    "\n",
    "        # calculating the number of patches\n",
    "        self.n_patches=(self.img_size[0]*self.img_size[1])//(self.patch_size[0]*self.patch_size[1])\n",
    "        \n",
    "        # calculating the max sequence length \n",
    "        self.max_sequence_length=self.n_patches+1\n",
    "        \n",
    "        # Bring all layers together\n",
    "        # Adding Patch Embedding Layer\n",
    "        self.patch=PatchEmbedding(self.d_model,self.img_size,self.patch_size,self.n_channels)\n",
    "        # Adding Positional Embedding Layer\n",
    "        self.positional=PositionalEmbedding(self.max_sequence_length,self.d_model)\n",
    "        # Adding Transformer Encoder Layer\n",
    "        self.encoder=nn.Sequential(*[TransformerEncoder( self.d_model, self.n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        # Classification Head\n",
    "        self.classification=nn.Sequential(\n",
    "            nn.Linear(self.d_model,self.n_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.patch(x)\n",
    "        x=self.positional(x)\n",
    "        x=self.encoder(x)\n",
    "        x=self.classification(x[:,0])\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f718f2-f237-4243-96c0-38a359702fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 1280\n",
    "n_classes = 4\n",
    "img_size = (224,224)\n",
    "patch_size = (14,14)\n",
    "n_channels = 3\n",
    "n_heads = 16\n",
    "n_layers = 32\n",
    "epochs = 5\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab829c2c-3628-492f-94ad-6f53bad762b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30be0f35-3254-4aa0-969b-0180d116dfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b722b37c-bd23-4ee6-b641-4630294ee2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Vision(d_model,n_classes,img_size,patch_size,n_channels,n_heads,n_layers)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75121d1d-148c-46cc-9e45-3a0e49219f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model,lr=1e-4,epochs=1,save_dir=\"./results\"):\n",
    "    # create dir if not exsists\n",
    "    os.makedirs(save_dir,exist_ok=True)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    #add learning rate scheduler\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.1,patience=5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_metrics_history = [] \n",
    "    val_metrics_history = []\n",
    "    best_metrics = {\n",
    "        'val_loss': float('inf'),\n",
    "        'val_top1': 0.0,\n",
    "        'val_top3': 0.0,\n",
    "        'epoch': 0\n",
    "    }\n",
    "   \n",
    "    #try to load the latest checkpoint\n",
    "    latest_checkpoint=find_latest_checkpoint(save_dir)\n",
    "    start_epoch=0\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        start_epoch = load_checkpoint(model, optimizer, scheduler, latest_checkpoint)\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        \n",
    "        # Load metrics history if available\n",
    "        metrics_file = os.path.join(save_dir, \"results.json\")\n",
    "        if os.path.exists(metrics_file):\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "                train_losses = results.get(\"train_losses\", [])\n",
    "                val_losses = results.get(\"val_losses\", [])\n",
    "                train_metrics_history = results.get(\"train_metrics_history\", [])\n",
    "                val_metrics_history = results.get(\"val_metrics_history\", [])\n",
    "                \n",
    "                # Load best metrics from history\n",
    "                if val_metrics_history:\n",
    "                    best_epoch_idx = min(range(len(val_metrics_history)), \n",
    "                                       key=lambda i: val_metrics_history[i].get(\"loss\", float('inf')))\n",
    "                    best_metrics = {\n",
    "                        'val_loss': val_metrics_history[best_epoch_idx].get(\"loss\", float('inf')),\n",
    "                        'val_top1': val_metrics_history[best_epoch_idx].get(\"top1_accuracy\", 0.0),\n",
    "                        'val_top3': val_metrics_history[best_epoch_idx].get(\"top3_accuracy\", 0.0),\n",
    "                        'epoch': best_epoch_idx\n",
    "                    }\n",
    "                print(f\"Loaded metrics history from previous training\")\n",
    "    \n",
    "    \n",
    "    training_start_time=time.time()\n",
    "    \n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        model.train()\n",
    "        running_loss=0\n",
    "        epoch_start_time=time.time()\n",
    "        train_y_true, train_y_pred, train_y_score = [], [], []\n",
    "        for inputs,labels in tqdm(train_loader):\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            output=model(inputs)\n",
    "            loss=criterion(output,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "            _,predicted=torch.max(output,1)\n",
    "            train_y_true.extend(labels.cpu().numpy())\n",
    "            train_y_pred.extend(predicted.cpu().numpy())\n",
    "            train_y_score.extend(torch.softmax(output, dim=1).detach().cpu().numpy())\n",
    "            \n",
    "        training_loss=running_loss/len(train_loader)\n",
    "        train_losses.append(training_loss)\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_metrics = calculate_metrics(train_y_true, train_y_pred, np.array(train_y_score))\n",
    "        train_metrics['loss'] = training_loss  # Add loss to the metrics dictionary\n",
    "        train_metrics_history.append(train_metrics)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss=0\n",
    "        val_y_true, val_y_pred, val_y_score = [], [], []\n",
    "        with torch.inference_mode():\n",
    "            for inputs,labels in tqdm(val_loader):\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                output=model(inputs)\n",
    "                loss=criterion(output,labels)\n",
    "                running_loss+=loss.item()\n",
    "                _,predicted=torch.max(output,1)\n",
    "                val_y_true.extend(labels.cpu().numpy())\n",
    "                val_y_pred.extend(predicted.cpu().numpy())\n",
    "                val_y_score.extend(torch.softmax(output, dim=1).detach().cpu().numpy())\n",
    "                \n",
    "        val_loss=running_loss/len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_metrics=calculate_metrics(val_y_true,val_y_pred,np.array(val_y_score))\n",
    "        val_metrics['loss'] = val_loss  # Add loss to the metrics dictionary\n",
    "        val_metrics_history.append(val_metrics)\n",
    "\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {training_loss:.4f}\")\n",
    "        print(f\"  Train Top-1: {train_metrics['top_1_accuracy']*100:.2f}%\")\n",
    "        print(f\"  Train Top-3: {train_metrics['top_3_accuracy']*100:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Top-1: {val_metrics['top_1_accuracy']*100:.2f}%\")\n",
    "        print(f\"  Val Top-3: {val_metrics['top_3_accuracy']*100:.2f}%\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss<best_metrics['val_loss']:\n",
    "            best_metrics['val_loss']=val_loss\n",
    "            best_metrics['val_top1']=val_metrics['top_1_accuracy']\n",
    "            best_metrics['val_top3']=val_metrics['top_3_accuracy']\n",
    "            checkpoint_path=os.path.join(save_dir,\"best_model_checkpoint.pth\")\n",
    "            torch.save({\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict(),\n",
    "                'loss':val_loss,\n",
    "                'metrics':val_metrics,                \n",
    "            },checkpoint_path)\n",
    "            print(f\"  Regular checkpoint saved for epoch {epoch+1}\")\n",
    "            \n",
    "        if (epoch+1)%10==0:\n",
    "            checkpoint_path=os.path.join(save_dir,f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            torch.save({\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict(),\n",
    "                'loss':val_loss,\n",
    "                'metrics':val_metrics\n",
    "            },checkpoint_path)\n",
    "            print(f\"Reguler Checkpoint saved for epoch {epoch+1}\")\n",
    "        # Save intermediate results after each epoch\n",
    "        results = {\n",
    "            \"train_losses\": [m['loss'] for m in train_metrics_history],\n",
    "            \"val_losses\": [m['loss'] for m in val_metrics_history],\n",
    "            \"train_metrics_history\": train_metrics_history,\n",
    "            \"val_metrics_history\": val_metrics_history,\n",
    "            \"best_validation\": {\n",
    "                \"epoch\": best_metrics['epoch'] + 1,\n",
    "                \"metrics\": val_metrics_history[best_metrics['epoch']] if best_metrics['epoch'] < len(val_metrics_history) else val_metrics_history[-1]\n",
    "            }\n",
    "        }\n",
    "        results_path = os.path.join(save_dir, \"results.json\")\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)    \n",
    "\n",
    "    # Calculate total training time\n",
    "    total_training_time=time.time()-training_start_time\n",
    "    training_time_formatted = str(datetime.timedelta(seconds=int(total_training_time)))\n",
    "    print(f\"\\nTotal training time: {training_time_formatted}\")\n",
    "\n",
    "    #Testing loop\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    test_y_true,test_y_pred,test_y_score=[],[],[]\n",
    "    with torch.inference_mode():\n",
    "        for inputs,labels in tqdm(test_loader):\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            output=model(inputs)\n",
    "            loss=criterion(output,labels)\n",
    "            test_loss+=loss.item()\n",
    "            _,predicted=torch.max(output,1)\n",
    "            test_y_true.extend(labels.cpu().numpy())\n",
    "            test_y_pred.extend(predicted.cpu().numpy())\n",
    "            test_y_score.extend(torch.softmax(output,dim=1).detach().cpu().numpy())\n",
    "    test_loss/=len(test_loader)\n",
    "    test_metrics=calculate_metrics(test_y_true,test_y_pred,np.array(test_y_score))\n",
    "\n",
    "    # Print final test results\n",
    "    print(\"\\nFinal Test Results:\")\n",
    "    print(f\"  Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Top-1: {test_metrics['top_1_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Top-3: {test_metrics['top_3_accuracy']*100:.2f}%\")\n",
    "\n",
    "    # Save all metrics history\n",
    "    metrics_history = {\n",
    "        'train': train_metrics_history,\n",
    "        'val': val_metrics_history,\n",
    "        'test': test_metrics,\n",
    "        'best_validation': {\n",
    "            'epoch': best_metrics['epoch'],\n",
    "            'metrics': val_metrics_history[best_metrics['epoch']]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    #plot comparison btw top_1_acc and top_3_acc\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # Plot comparison of top-1 and top-3 accuracies\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Use the actual length of history instead of num_epochs\n",
    "    actual_epochs = len(train_metrics_history)\n",
    "    epochs = range(1, actual_epochs + 1)\n",
    "    train_top1 = [m['top_1_accuracy'] * 100 for m in train_metrics_history]\n",
    "    train_top3 = [m['top_3_accuracy'] * 100 for m in train_metrics_history]\n",
    "    val_top1 = [m['top_1_accuracy'] * 100 for m in val_metrics_history]\n",
    "    val_top3 = [m['top_3_accuracy'] * 100 for m in val_metrics_history]\n",
    "    plt.plot(epochs, train_top1, 'b-', label='Train Top-1')\n",
    "    plt.plot(epochs, train_top3, 'b--', label='Train Top-3')\n",
    "    plt.plot(epochs, val_top1, 'r-', label='Val Top-1')\n",
    "    plt.plot(epochs, val_top3, 'r--', label='Val Top-3')\n",
    "    plt.title('Top-1 and Top-3 Accuracies Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_dir, 'top1_top3_comparison.png'))\n",
    "    plt.close()\n",
    "\n",
    "    #plot loss curves\n",
    "    plt.figure(figsize=(10,6))\n",
    "    train_losses = [epoch_data['loss'] for epoch_data in train_metrics_history]\n",
    "    val_losses = [epoch_data['loss'] for epoch_data in val_metrics_history]  \n",
    "    plt.plot(epochs,train_losses,'b-',label='Train Loss')\n",
    "    plt.plot(epochs,val_losses,'r-',label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_dir,'loss_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion matrix plot for test data\n",
    "    conf_matrix = confusion_matrix(test_y_true, test_y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Test Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    conf_matrix_path=os.path.join(save_dir, \"test_confusion_matrix.png\")\n",
    "    plt.savefig(conf_matrix_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Confusion matrix plot for training data\n",
    "    train_conf_matrix=confusion_matrix(train_y_true,train_y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues')\n",
    "    plt.title(\"Training Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    train_conf_matrix_path=os.path.join(save_dir,\"train_confusion_matrix.png\")\n",
    "    plt.savefig(train_conf_matrix_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save classification Reports\n",
    "    train_y_true_all = []\n",
    "    train_y_pred_all = []\n",
    "    epochs=1\n",
    "    for epoch in range(epochs):\n",
    "        train_y_true_all.extend(train_metrics_history[epoch].get('y_true', []))\n",
    "        train_y_pred_all.extend(train_metrics_history[epoch].get('y_pred', []))\n",
    "    \n",
    "    # If we don't have the raw predictions stored in metrics history, use the last epoch's data\n",
    "    if not train_y_true_all:\n",
    "        train_cls_report=classification_report(train_y_true,train_y_pred)\n",
    "    else:\n",
    "        train_cls_report=classification_report(train_y_true_all,train_y_pred_all)\n",
    "    \n",
    "    test_cls_report=classification_report(test_y_true,test_y_pred)\n",
    "    \n",
    "    train_report_path = os.path.join(save_dir,\"train_classification_report.txt\")\n",
    "    with open(train_report_path,\"w\") as f:\n",
    "        f.write(train_cls_report)\n",
    "    \n",
    "    test_report_path=os.path.join(save_dir,\"test_classification_report.txt\")\n",
    "    with open(test_report_path,\"w\") as f:\n",
    "        f.write(test_cls_report)\n",
    "\n",
    "    # Save detailed metrics\n",
    "    metrics_path = os.path.join(save_dir, \"detailed_metrics.txt\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        f.write(\"TRAINING METRICS (Final Epoch):\\n\")\n",
    "        f.write(\"=============================\\n\")\n",
    "        for metric, value in train_metrics_history[-1].items():\n",
    "            if value is not None:\n",
    "                f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric}: N/A\\n\")\n",
    "        \n",
    "        f.write(\"\\nVALIDATION METRICS (Best Epoch):\\n\")\n",
    "        f.write(\"==============================\\n\")\n",
    "        best_val_metrics = val_metrics_history[best_metrics['epoch']]\n",
    "        for metric, value in best_val_metrics.items():\n",
    "            if value is not None:\n",
    "                f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric}: N/A\\n\")\n",
    "        \n",
    "        f.write(\"\\nTEST METRICS:\\n\")\n",
    "        f.write(\"=============\\n\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            if value is not None:\n",
    "                f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric}: N/A\\n\")\n",
    "        \n",
    "        # Add training time information\n",
    "        f.write(\"\\nTRAINING TIME:\\n\")\n",
    "        f.write(\"=============\\n\")\n",
    "        f.write(f\"Total training time: {training_time_formatted}\\n\")\n",
    "        f.write(f\"Average time per epoch: {total_training_time/epochs:.2f} seconds\\n\")\n",
    "\n",
    "    # Save epoch wise data\n",
    "    epoch_metric_path=os.path.join(save_dir,\"training_metrics.txt\")\n",
    "    with open(epoch_metric_path,\"w\") as f:\n",
    "        f.write(\"Epoch wise Training and Validation Metrics\\n\")\n",
    "        for i, epoch_idx in enumerate(range(len(train_metrics_history))):\n",
    "            actual_epoch = start_epoch + i  # Calculate the true epoch number\n",
    "            f.write(f\"Epoch {actual_epoch+1}:\\n\")\n",
    "            f.write(f\"  Train Loss: {train_metrics_history[epoch_idx]['loss']:.4f}\\n\")\n",
    "            f.write(f\"  Train Top-1: {train_metrics_history[epoch_idx]['top_1_accuracy']*100:.2f}%\\n\")\n",
    "            f.write(f\"  Train Top-3: {train_metrics_history[epoch_idx]['top_3_accuracy']*100:.2f}%\\n\")\n",
    "            f.write(f\"  Val Loss: {val_metrics_history[epoch_idx]['loss']:.4f}\\n\")\n",
    "            f.write(f\"  Val Top-1: {val_metrics_history[epoch_idx]['top_1_accuracy']*100:.2f}%\\n\")\n",
    "            f.write(f\"  Val Top-3: {val_metrics_history[epoch_idx]['top_3_accuracy']*100:.2f}%\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(f\"Final Test Loss: {test_loss:.4f}\\n\")\n",
    "        f.write(f\"Test Top-1: {test_metrics['top_1_accuracy']*100:.2f}%\\n\")\n",
    "        f.write(f\"Test Top-3: {test_metrics['top_3_accuracy']*100:.2f}%\\n\")\n",
    "        f.write(f\"Total training time: {training_time_formatted}\\n\")\n",
    "\n",
    "    # Compute and Save model Statistics\n",
    "    model_stats = compute_model_statistics(model, input_size=(3, 224, 224), device=device)\n",
    "    stats_path = os.path.join(save_dir, \"model_statistics.json\")\n",
    "    with open(stats_path, \"w\") as f:\n",
    "        def convert_to_serializable(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_to_serializable(item) for item in list(obj)]\n",
    "            elif isinstance(obj, (np.float32, np.float64)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, (np.int32, np.int64)):\n",
    "                return int(obj)\n",
    "            elif obj is None:\n",
    "                return None\n",
    "            return obj\n",
    "        \n",
    "        json.dump(convert_to_serializable(model_stats), f, indent=4)\n",
    "\n",
    "    # Compile all results in a dictionary and save to JSON\n",
    "    results = {\n",
    "        \"train_losses\": [m['loss'] for m in train_metrics_history],\n",
    "        \"val_losses\": [m['loss'] for m in val_metrics_history],\n",
    "        \"train_top1_accuracies\": train_top1,\n",
    "        \"train_top3_accuracies\": train_top3,\n",
    "        \"val_top1_accuracies\": val_top1,\n",
    "        \"val_top3_accuracies\": val_top3,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_metrics\": convert_to_serializable(test_metrics),\n",
    "        \"best_validation\": {\n",
    "            \"epoch\": best_metrics['epoch'] + 1,\n",
    "            \"metrics\": convert_to_serializable(val_metrics_history[best_metrics['epoch']] if best_metrics['epoch'] < len(val_metrics_history) else val_metrics_history[-1])\n",
    "        },\n",
    "        \"model_statistics\": convert_to_serializable(model_stats),\n",
    "        \"training_time\": {\n",
    "            \"total_seconds\": total_training_time,\n",
    "            \"formatted\": training_time_formatted,\n",
    "            \"average_epoch_seconds\": total_training_time/epochs\n",
    "        },\n",
    "        \"plots\": {\n",
    "            \"top1_top3_comparison\": os.path.join(save_dir, 'top1_top3_comparison.png'),\n",
    "            \"loss_curves\": os.path.join(save_dir, 'loss_curves.png'),\n",
    "            \"test_confusion_matrix\": conf_matrix_path,\n",
    "            \"train_confusion_matrix\": train_conf_matrix_path\n",
    "        }\n",
    "    }\n",
    "    results_path = os.path.join(save_dir, \"results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    # Save the trained model\n",
    "    model_save_path = os.path.join(save_dir, \"trained_model.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Trained model saved to {model_save_path}\")\n",
    "    print(f\"All outputs have been saved to {os.path.abspath(save_dir)}\")\n",
    "\n",
    "    return model, metrics_history   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e777c1d1-9bda-434b-be84-17f6cddb25bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [03:07<00:00, 15.65s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:42<00:00, 14.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5:\n",
      "  Train Loss: 1.3841\n",
      "  Train Top-1: 23.78%\n",
      "  Train Top-3: 72.70%\n",
      "  Val Loss: 1.3742\n",
      "  Val Top-1: 24.71%\n",
      "  Val Top-3: 70.59%\n",
      "  Regular checkpoint saved for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [03:05<00:00, 15.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:44<00:00, 14.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5:\n",
      "  Train Loss: 1.3247\n",
      "  Train Top-1: 38.11%\n",
      "  Train Top-3: 87.30%\n",
      "  Val Loss: 1.2724\n",
      "  Val Top-1: 38.82%\n",
      "  Val Top-3: 88.24%\n",
      "  Regular checkpoint saved for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [03:08<00:00, 15.68s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:45<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5:\n",
      "  Train Loss: 1.3121\n",
      "  Train Top-1: 40.00%\n",
      "  Train Top-3: 88.38%\n",
      "  Val Loss: 1.3385\n",
      "  Val Top-1: 30.59%\n",
      "  Val Top-3: 84.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [03:17<00:00, 16.44s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:43<00:00, 14.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5:\n",
      "  Train Loss: 1.3000\n",
      "  Train Top-1: 40.81%\n",
      "  Train Top-3: 87.84%\n",
      "  Val Loss: 1.2503\n",
      "  Val Top-1: 37.65%\n",
      "  Val Top-3: 94.12%\n",
      "  Regular checkpoint saved for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [03:10<00:00, 15.83s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:44<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5:\n",
      "  Train Loss: 1.2369\n",
      "  Train Top-1: 51.35%\n",
      "  Train Top-3: 91.08%\n",
      "  Val Loss: 1.1939\n",
      "  Val Top-1: 57.65%\n",
      "  Val Top-3: 90.59%\n",
      "  Regular checkpoint saved for epoch 5\n",
      "\n",
      "Total training time: 0:19:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:22<00:00, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "  Loss: 1.2461\n",
      "  Top-1: 62.50%\n",
      "  Top-3: 85.42%\n",
      "ptflops package not found. Skipping FLOPs calculation.\n",
      "Trained model saved to C:\\Users\\Hp\\Desktop\\CS Projects\\PyTorch\\VGG16\\VIT Transformer\\results\\trained_model.pth\n",
      "All outputs have been saved to C:\\Users\\Hp\\Desktop\\CS Projects\\PyTorch\\VGG16\\VIT Transformer\\results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Vision(\n",
       "   (patch): PatchEmbedding(\n",
       "     (layer): Conv2d(3, 9, kernel_size=(16, 16), stride=(16, 16))\n",
       "   )\n",
       "   (positional): PositionalEmbedding()\n",
       "   (encoder): Sequential(\n",
       "     (0): TransformerEncoder(\n",
       "       (ln1): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "       (mha): MultiHeadAttention(\n",
       "         (W_o): Linear(in_features=9, out_features=9, bias=True)\n",
       "         (heads): ModuleList(\n",
       "           (0-2): 3 x AttentionHead(\n",
       "             (query): Linear(in_features=9, out_features=3, bias=True)\n",
       "             (key): Linear(in_features=9, out_features=3, bias=True)\n",
       "             (value): Linear(in_features=9, out_features=3, bias=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln2): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): Sequential(\n",
       "         (0): Linear(in_features=9, out_features=36, bias=True)\n",
       "         (1): GELU(approximate='none')\n",
       "         (2): Linear(in_features=36, out_features=9, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (1): TransformerEncoder(\n",
       "       (ln1): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "       (mha): MultiHeadAttention(\n",
       "         (W_o): Linear(in_features=9, out_features=9, bias=True)\n",
       "         (heads): ModuleList(\n",
       "           (0-2): 3 x AttentionHead(\n",
       "             (query): Linear(in_features=9, out_features=3, bias=True)\n",
       "             (key): Linear(in_features=9, out_features=3, bias=True)\n",
       "             (value): Linear(in_features=9, out_features=3, bias=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln2): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): Sequential(\n",
       "         (0): Linear(in_features=9, out_features=36, bias=True)\n",
       "         (1): GELU(approximate='none')\n",
       "         (2): Linear(in_features=36, out_features=9, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (2): TransformerEncoder(\n",
       "       (ln1): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "       (mha): MultiHeadAttention(\n",
       "         (W_o): Linear(in_features=9, out_features=9, bias=True)\n",
       "         (heads): ModuleList(\n",
       "           (0-2): 3 x AttentionHead(\n",
       "             (query): Linear(in_features=9, out_features=3, bias=True)\n",
       "             (key): Linear(in_features=9, out_features=3, bias=True)\n",
       "             (value): Linear(in_features=9, out_features=3, bias=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln2): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): Sequential(\n",
       "         (0): Linear(in_features=9, out_features=36, bias=True)\n",
       "         (1): GELU(approximate='none')\n",
       "         (2): Linear(in_features=36, out_features=9, bias=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classification): Sequential(\n",
       "     (0): Linear(in_features=9, out_features=4, bias=True)\n",
       "     (1): Softmax(dim=-1)\n",
       "   )\n",
       " ),\n",
       " {'train': [{'accuracy_score': 0.23783783783783785,\n",
       "    'top_1_accuracy': 0.23783783783783785,\n",
       "    'top_3_accuracy': 0.727027027027027,\n",
       "    'precision_micro': 0.23783783783783785,\n",
       "    'precision_macro': 0.14106570512820513,\n",
       "    'precision_weighted': 0.1499696812196812,\n",
       "    'recall_micro': 0.23783783783783785,\n",
       "    'recall_macro': 0.22083333333333333,\n",
       "    'recall_weighted': 0.23783783783783785,\n",
       "    'f1_micro': 0.23783783783783785,\n",
       "    'f1_macro': 0.15307223552526555,\n",
       "    'f1_weighted': 0.1641542998146424,\n",
       "    'auc_micro': np.float64(0.5223569515461408),\n",
       "    'auc_macro': np.float64(0.511550606641124),\n",
       "    'auc_weighted': np.float64(0.5106723965344655),\n",
       "    'loss': 1.3840779463450115},\n",
       "   {'accuracy_score': 0.3810810810810811,\n",
       "    'top_1_accuracy': 0.3810810810810811,\n",
       "    'top_3_accuracy': 0.8729729729729729,\n",
       "    'precision_micro': 0.3810810810810811,\n",
       "    'precision_macro': 0.5232682980599647,\n",
       "    'precision_weighted': 0.5012977262977263,\n",
       "    'recall_micro': 0.3810810810810811,\n",
       "    'recall_macro': 0.36236111111111113,\n",
       "    'recall_weighted': 0.3810810810810811,\n",
       "    'f1_micro': 0.3810810810810811,\n",
       "    'f1_macro': 0.31487098685647863,\n",
       "    'f1_weighted': 0.3279650107901131,\n",
       "    'auc_micro': np.float64(0.6737594351107865),\n",
       "    'auc_macro': np.float64(0.6716772942893633),\n",
       "    'auc_weighted': np.float64(0.6687617297100056),\n",
       "    'loss': 1.3246878782908122},\n",
       "   {'accuracy_score': 0.4,\n",
       "    'top_1_accuracy': 0.4,\n",
       "    'top_3_accuracy': 0.8837837837837837,\n",
       "    'precision_micro': 0.4,\n",
       "    'precision_macro': 0.45804119873425575,\n",
       "    'precision_weighted': 0.43195786218681304,\n",
       "    'recall_micro': 0.4,\n",
       "    'recall_macro': 0.3920138888888889,\n",
       "    'recall_weighted': 0.4,\n",
       "    'f1_micro': 0.4,\n",
       "    'f1_macro': 0.27225204278067294,\n",
       "    'f1_weighted': 0.2798620156425337,\n",
       "    'auc_micro': np.float64(0.6653761869978085),\n",
       "    'auc_macro': np.float64(0.6292067141032659),\n",
       "    'auc_weighted': np.float64(0.6248328624190693),\n",
       "    'loss': 1.3120786746342976},\n",
       "   {'accuracy_score': 0.4081081081081081,\n",
       "    'top_1_accuracy': 0.4081081081081081,\n",
       "    'top_3_accuracy': 0.8783783783783784,\n",
       "    'precision_micro': 0.4081081081081081,\n",
       "    'precision_macro': 0.3203467593934125,\n",
       "    'precision_weighted': 0.3172421624754282,\n",
       "    'recall_micro': 0.4081081081081081,\n",
       "    'recall_macro': 0.42118055555555556,\n",
       "    'recall_weighted': 0.4081081081081081,\n",
       "    'f1_micro': 0.4081081081081081,\n",
       "    'f1_macro': 0.3540842911877395,\n",
       "    'f1_weighted': 0.34807000103551833,\n",
       "    'auc_micro': np.float64(0.6759057706355004),\n",
       "    'auc_macro': np.float64(0.6746091611932128),\n",
       "    'auc_weighted': np.float64(0.6700695276557345),\n",
       "    'loss': 1.2999675472577412},\n",
       "   {'accuracy_score': 0.5135135135135135,\n",
       "    'top_1_accuracy': 0.5135135135135135,\n",
       "    'top_3_accuracy': 0.9108108108108108,\n",
       "    'precision_micro': 0.5135135135135135,\n",
       "    'precision_macro': 0.40264192508642876,\n",
       "    'precision_weighted': 0.3872235808369519,\n",
       "    'recall_micro': 0.5135135135135135,\n",
       "    'recall_macro': 0.5227083333333333,\n",
       "    'recall_weighted': 0.5135135135135135,\n",
       "    'f1_micro': 0.5135135135135135,\n",
       "    'f1_macro': 0.4453450459432513,\n",
       "    'f1_weighted': 0.43286612041159206,\n",
       "    'auc_micro': np.float64(0.7351521792062332),\n",
       "    'auc_macro': np.float64(0.743643666757891),\n",
       "    'auc_weighted': np.float64(0.7383021445090411),\n",
       "    'loss': 1.2368804315725963}],\n",
       "  'val': [{'accuracy_score': 0.24705882352941178,\n",
       "    'top_1_accuracy': 0.24705882352941178,\n",
       "    'top_3_accuracy': 0.7058823529411765,\n",
       "    'precision_micro': 0.24705882352941178,\n",
       "    'precision_macro': 0.16666666666666669,\n",
       "    'precision_weighted': 0.1568627450980392,\n",
       "    'recall_micro': 0.24705882352941178,\n",
       "    'recall_macro': 0.26249999999999996,\n",
       "    'recall_weighted': 0.24705882352941178,\n",
       "    'f1_micro': 0.24705882352941178,\n",
       "    'f1_macro': 0.17777777777777778,\n",
       "    'f1_weighted': 0.16732026143790849,\n",
       "    'auc_micro': np.float64(0.5447750865051904),\n",
       "    'auc_macro': np.float64(0.6492948717948719),\n",
       "    'auc_weighted': np.float64(0.6495324283559577),\n",
       "    'loss': 1.3741634686787922},\n",
       "   {'accuracy_score': 0.38823529411764707,\n",
       "    'top_1_accuracy': 0.38823529411764707,\n",
       "    'top_3_accuracy': 0.8823529411764706,\n",
       "    'precision_micro': 0.38823529411764707,\n",
       "    'precision_macro': 0.20017482517482516,\n",
       "    'precision_weighted': 0.188399835458659,\n",
       "    'recall_micro': 0.38823529411764707,\n",
       "    'recall_macro': 0.4125,\n",
       "    'recall_weighted': 0.38823529411764707,\n",
       "    'f1_micro': 0.38823529411764707,\n",
       "    'f1_macro': 0.2665094339622641,\n",
       "    'f1_weighted': 0.25083240843507215,\n",
       "    'auc_micro': np.float64(0.7077739331026528),\n",
       "    'auc_macro': np.float64(0.7197435897435898),\n",
       "    'auc_weighted': np.float64(0.7207390648567119),\n",
       "    'loss': 1.272361437479655},\n",
       "   {'accuracy_score': 0.3058823529411765,\n",
       "    'top_1_accuracy': 0.3058823529411765,\n",
       "    'top_3_accuracy': 0.8470588235294118,\n",
       "    'precision_micro': 0.3058823529411765,\n",
       "    'precision_macro': 0.20393120393120392,\n",
       "    'precision_weighted': 0.19193525075878015,\n",
       "    'recall_micro': 0.3058823529411765,\n",
       "    'recall_macro': 0.325,\n",
       "    'recall_weighted': 0.3058823529411765,\n",
       "    'f1_micro': 0.3058823529411765,\n",
       "    'f1_macro': 0.20315717227179136,\n",
       "    'f1_weighted': 0.1912067503734507,\n",
       "    'auc_micro': np.float64(0.6531026528258362),\n",
       "    'auc_macro': np.float64(0.7176538461538462),\n",
       "    'auc_weighted': np.float64(0.7287330316742081),\n",
       "    'loss': 1.3384812275568645},\n",
       "   {'accuracy_score': 0.3764705882352941,\n",
       "    'top_1_accuracy': 0.3764705882352941,\n",
       "    'top_3_accuracy': 0.9411764705882353,\n",
       "    'precision_micro': 0.3764705882352941,\n",
       "    'precision_macro': 0.4399114484503479,\n",
       "    'precision_weighted': 0.4728578338356216,\n",
       "    'recall_micro': 0.3764705882352941,\n",
       "    'recall_macro': 0.39749999999999996,\n",
       "    'recall_weighted': 0.3764705882352941,\n",
       "    'f1_micro': 0.3764705882352941,\n",
       "    'f1_macro': 0.2706806449557071,\n",
       "    'f1_weighted': 0.2592831409537877,\n",
       "    'auc_micro': np.float64(0.6971626297577855),\n",
       "    'auc_macro': np.float64(0.7082564102564103),\n",
       "    'auc_weighted': np.float64(0.7151432880844646),\n",
       "    'loss': 1.2503431836764018},\n",
       "   {'accuracy_score': 0.5764705882352941,\n",
       "    'top_1_accuracy': 0.5764705882352941,\n",
       "    'top_3_accuracy': 0.9058823529411765,\n",
       "    'precision_micro': 0.5764705882352941,\n",
       "    'precision_macro': 0.5920634920634921,\n",
       "    'precision_weighted': 0.6101774042950514,\n",
       "    'recall_micro': 0.5764705882352941,\n",
       "    'recall_macro': 0.5675,\n",
       "    'recall_weighted': 0.5764705882352941,\n",
       "    'f1_micro': 0.5764705882352941,\n",
       "    'f1_macro': 0.5398383825165217,\n",
       "    'f1_weighted': 0.5551420070743733,\n",
       "    'auc_micro': np.float64(0.8200230680507496),\n",
       "    'auc_macro': np.float64(0.8372307692307692),\n",
       "    'auc_weighted': np.float64(0.8445701357466064),\n",
       "    'loss': 1.193906267484029}],\n",
       "  'test': {'accuracy_score': 0.625,\n",
       "   'top_1_accuracy': 0.625,\n",
       "   'top_3_accuracy': 0.8541666666666666,\n",
       "   'precision_micro': 0.625,\n",
       "   'precision_macro': 0.7083333333333334,\n",
       "   'precision_weighted': 0.7048611111111112,\n",
       "   'recall_micro': 0.625,\n",
       "   'recall_macro': 0.575,\n",
       "   'recall_weighted': 0.625,\n",
       "   'f1_micro': 0.625,\n",
       "   'f1_macro': 0.5661268556005398,\n",
       "   'f1_weighted': 0.5983749437696807,\n",
       "   'auc_micro': np.float64(0.7876157407407407),\n",
       "   'auc_macro': np.float64(0.7876794258373205),\n",
       "   'auc_weighted': np.float64(0.7965510366826155)},\n",
       "  'best_validation': {'epoch': 0,\n",
       "   'metrics': {'accuracy_score': 0.24705882352941178,\n",
       "    'top_1_accuracy': 0.24705882352941178,\n",
       "    'top_3_accuracy': 0.7058823529411765,\n",
       "    'precision_micro': 0.24705882352941178,\n",
       "    'precision_macro': 0.16666666666666669,\n",
       "    'precision_weighted': 0.1568627450980392,\n",
       "    'recall_micro': 0.24705882352941178,\n",
       "    'recall_macro': 0.26249999999999996,\n",
       "    'recall_weighted': 0.24705882352941178,\n",
       "    'f1_micro': 0.24705882352941178,\n",
       "    'f1_macro': 0.17777777777777778,\n",
       "    'f1_weighted': 0.16732026143790849,\n",
       "    'auc_micro': np.float64(0.5447750865051904),\n",
       "    'auc_macro': np.float64(0.6492948717948719),\n",
       "    'auc_weighted': np.float64(0.6495324283559577),\n",
       "    'loss': 1.3741634686787922}}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir=r\"C:\\Users\\Hp\\Desktop\\CS Projects\\PyTorch\\VGG16\\VIT Transformer\\results\"\n",
    "train_eval(model,lr,epochs,save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3365fb6-9f07-4444-b703-e42a15f32597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24185737-3b77-44c2-8896-aca5f44c005a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bcbed-c024-4fc7-bec4-888156e05c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d1c75-1b44-402f-8693-bf0827b174a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc52e6-fa09-463d-aa11-9e22279701cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de90117-0bb3-42ac-91a0-843a79f66d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d582642-48ee-4d44-8bad-ad05e757e77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d31f5a-a30d-4f33-9328-ab061506722a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca3084-5489-46df-b0d3-100227a655f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "406b797d-decf-4368-a7a9-278b40e17c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Vision                                   --\n",
       "├─PatchEmbedding: 1-1                    --\n",
       "│    └─Conv2d: 2-1                       6,921\n",
       "├─PositionalEmbedding: 1-2               9\n",
       "├─TransformerEncoder: 1-3                --\n",
       "│    └─LayerNorm: 2-2                    18\n",
       "│    └─MultiHeadAttention: 2-3           --\n",
       "│    │    └─Linear: 3-1                  90\n",
       "│    │    └─ModuleList: 3-2              270\n",
       "│    └─LayerNorm: 2-4                    18\n",
       "│    └─Sequential: 2-5                   --\n",
       "│    │    └─Linear: 3-3                  360\n",
       "│    │    └─GELU: 3-4                    --\n",
       "│    │    └─Linear: 3-5                  333\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─Linear: 2-6                       40\n",
       "│    └─Softmax: 2-7                      --\n",
       "=================================================================\n",
       "Total params: 8,059\n",
       "Trainable params: 8,059\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c6da7-2f73-4dd4-a323-3c9b43620399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e32a8c-5753-473a-be8b-ed3bc069b2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47357e45-e916-4358-81c2-2a6e30f6b381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf456d4-030c-4330-8974-4f81d8313a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449a7fc-0ad3-4d24-9445-10b25b9ce8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482fa11-a5ba-416b-be14-d1f9b7776a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d941e16-5018-4d0c-9587-ea1c3cb9127f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e406fe-10fd-4130-a40f-e467a6d24f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b55836-6c63-42e8-a781-bf13a287d6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b24762-3b49-4ef2-885c-f249a46427c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7125-ad28-4e3b-9ea0-91147cb32cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547dfb6-25a7-4804-b0bc-34c9f6c0b54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
